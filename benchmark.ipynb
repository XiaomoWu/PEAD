{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('vegafusion')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-03 04:11:39 AM] [INFO] Working directory: /home/yu/chaoyang/projects/Call/call/code/v4/reproduce-finetune\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import textwrap\n",
    "from dataclasses import asdict, dataclass\n",
    "from datetime import date, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from polars import col as c\n",
    "from polars import lit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm import tqdm\n",
    "\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "# Configure logging to show timestamp, log level and message\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s] [%(levelname)s] %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S %p\",\n",
    ")\n",
    "\n",
    "# set the number of rows to display\n",
    "pl.Config.set_tbl_rows(1000)\n",
    "\n",
    "work_dir = os.getcwd()\n",
    "logging.info(f\"Working directory: {work_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to evaluate the classification performance\n",
    "\n",
    "def evaluate_classification(yt, models, n_classes=5):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance with multiple metrics for multiple models.\n",
    "    Returns results as a polars DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        ytx (pl.DataFrame): DataFrame containing predictions and ground truth\n",
    "        models (list): List of model names to evaluate\n",
    "        n_classes (int): Number of classes (default=5)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model in models:\n",
    "        data = yt.filter(pl.col(\"model\") == model)\n",
    "        t = np.array(data['t'])\n",
    "        y = np.array(data['y'])\n",
    "        \n",
    "        # Remove null values and corresponding targets\n",
    "        valid_mask = ~np.isnan(y)\n",
    "        y = y[valid_mask]\n",
    "        t = t[valid_mask]\n",
    "        \n",
    "        # number of non-null predictions\n",
    "        n_instances = len(y)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(t == y)\n",
    "        \n",
    "        # Calculate per-class precision, recall, and f1\n",
    "        f1 = []\n",
    "        support = []\n",
    "        auc_scores = []\n",
    "        accuracies = []\n",
    "        \n",
    "        # For micro-AUC: binarize labels for all classes at once\n",
    "        y_bin = label_binarize(y, classes=range(1, n_classes + 1))\n",
    "        t_bin = label_binarize(t, classes=range(1, n_classes + 1))\n",
    "        \n",
    "        # Calculate micro-AUC\n",
    "        micro_auc = roc_auc_score(t_bin, y_bin, average='micro')\n",
    "        \n",
    "        for class_idx in range(1, n_classes + 1):\n",
    "            # Calculate per-class metrics directly using multi-class labels\n",
    "            prec, rec, f1_score, sup = precision_recall_fscore_support(\n",
    "                t, y, labels=[class_idx], average=None\n",
    "            )\n",
    "            \n",
    "            # Calculate per-class accuracy\n",
    "            class_mask = t == class_idx\n",
    "            class_accuracy = np.mean(y[class_mask] == t[class_mask]) if any(class_mask) else 0.0\n",
    "            accuracies.append(class_accuracy)\n",
    "            \n",
    "            f1.append(float(f1_score[0]))\n",
    "            support.append(sup[0])\n",
    "            \n",
    "            # Calculate AUC using one-vs-rest approach\n",
    "            try:\n",
    "                # Get probability scores for this class\n",
    "                y_scores = (y == class_idx).astype(float)\n",
    "                t_binary = (t == class_idx).astype(int)\n",
    "                auc = roc_auc_score(t_binary, y_scores)\n",
    "                auc_scores.append(auc)\n",
    "            except:\n",
    "                auc_scores.append(np.nan)\n",
    "        \n",
    "        # Calculate macro metrics\n",
    "        macro_f1 = np.mean(f1)\n",
    "        micro_f1 = np.mean(t == y)\n",
    "        macro_auc = np.mean([x for x in auc_scores if not np.isnan(x)])\n",
    "        \n",
    "        # Add overall metrics\n",
    "        results.append({\n",
    "            \"model\": model,\n",
    "            \"metric_type\": \"overall\",\n",
    "            \"metric\": \"n_instances\",\n",
    "            \"value\": n_instances,\n",
    "            \"class\": None\n",
    "        })\n",
    "        results.append({\n",
    "            \"model\": model,\n",
    "            \"metric_type\": \"overall\",\n",
    "            \"metric\": \"accuracy\",\n",
    "            \"value\": accuracy,\n",
    "            \"class\": None\n",
    "        })\n",
    "        results.append({\n",
    "            \"model\": model,\n",
    "            \"metric_type\": \"overall\", \n",
    "            \"metric\": \"macro_f1\",\n",
    "            \"value\": macro_f1,\n",
    "            \"class\": None\n",
    "        })\n",
    "        results.append({\n",
    "            \"model\": model,\n",
    "            \"metric_type\": \"overall\",\n",
    "            \"metric\": \"micro_f1\",\n",
    "            \"value\": micro_f1,\n",
    "            \"class\": None\n",
    "        })\n",
    "        results.append({\n",
    "            \"model\": model,\n",
    "            \"metric_type\": \"overall\",\n",
    "            \"metric\": \"macro_auc\",\n",
    "            \"value\": macro_auc,\n",
    "            \"class\": None\n",
    "        })\n",
    "        results.append({\n",
    "            \"model\": model,\n",
    "            \"metric_type\": \"overall\",\n",
    "            \"metric\": \"micro_auc\",\n",
    "            \"value\": micro_auc,\n",
    "            \"class\": None\n",
    "        })\n",
    "        \n",
    "        # Add per-class metrics\n",
    "        for i in range(n_classes):\n",
    "            class_num = i + 1\n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"metric_type\": \"per_class\",\n",
    "                \"metric\": \"f1\",\n",
    "                \"value\": f1[i],\n",
    "                \"class\": class_num\n",
    "            })\n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"metric_type\": \"per_class\",\n",
    "                \"metric\": \"auc\",\n",
    "                \"value\": auc_scores[i],\n",
    "                \"class\": class_num\n",
    "            })\n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"metric_type\": \"per_class\",\n",
    "                \"metric\": \"accuracy\",\n",
    "                \"value\": accuracies[i],\n",
    "                \"class\": class_num\n",
    "            })\n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"metric_type\": \"per_class\",\n",
    "                \"metric\": \"support\",\n",
    "                \"value\": float(support[i]),\n",
    "                \"class\": class_num\n",
    "            })\n",
    "    \n",
    "    # Convert to polars DataFrame\n",
    "    results_df = pl.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>class</th><th>metric</th><th>value</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;ours&quot;</td><td>null</td><td>&quot;accuracy&quot;</td><td>0.262</td></tr><tr><td>&quot;ours&quot;</td><td>null</td><td>&quot;macro_auc&quot;</td><td>0.539</td></tr><tr><td>&quot;ours&quot;</td><td>null</td><td>&quot;macro_f1&quot;</td><td>0.261</td></tr><tr><td>&quot;ours&quot;</td><td>null</td><td>&quot;micro_auc&quot;</td><td>0.539</td></tr><tr><td>&quot;ours&quot;</td><td>null</td><td>&quot;micro_f1&quot;</td><td>0.262</td></tr><tr><td>&quot;ours&quot;</td><td>null</td><td>&quot;n_instances&quot;</td><td>3867.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6, 4)\n",
       "┌───────┬───────┬─────────────┬────────┐\n",
       "│ model ┆ class ┆ metric      ┆ value  │\n",
       "│ ---   ┆ ---   ┆ ---         ┆ ---    │\n",
       "│ str   ┆ i64   ┆ str         ┆ f64    │\n",
       "╞═══════╪═══════╪═════════════╪════════╡\n",
       "│ ours  ┆ null  ┆ accuracy    ┆ 0.262  │\n",
       "│ ours  ┆ null  ┆ macro_auc   ┆ 0.539  │\n",
       "│ ours  ┆ null  ┆ macro_f1    ┆ 0.261  │\n",
       "│ ours  ┆ null  ┆ micro_auc   ┆ 0.539  │\n",
       "│ ours  ┆ null  ┆ micro_f1    ┆ 0.262  │\n",
       "│ ours  ┆ null  ┆ n_instances ┆ 3867.0 │\n",
       "└───────┴───────┴─────────────┴────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>class</th><th>metric</th><th>value</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;ours&quot;</td><td>1</td><td>&quot;accuracy&quot;</td><td>0.336</td></tr><tr><td>&quot;ours&quot;</td><td>1</td><td>&quot;auc&quot;</td><td>0.584</td></tr><tr><td>&quot;ours&quot;</td><td>1</td><td>&quot;f1&quot;</td><td>0.332</td></tr><tr><td>&quot;ours&quot;</td><td>1</td><td>&quot;support&quot;</td><td>755.0</td></tr><tr><td>&quot;ours&quot;</td><td>2</td><td>&quot;accuracy&quot;</td><td>0.22</td></tr><tr><td>&quot;ours&quot;</td><td>2</td><td>&quot;auc&quot;</td><td>0.513</td></tr><tr><td>&quot;ours&quot;</td><td>2</td><td>&quot;f1&quot;</td><td>0.212</td></tr><tr><td>&quot;ours&quot;</td><td>2</td><td>&quot;support&quot;</td><td>697.0</td></tr><tr><td>&quot;ours&quot;</td><td>3</td><td>&quot;accuracy&quot;</td><td>0.208</td></tr><tr><td>&quot;ours&quot;</td><td>3</td><td>&quot;auc&quot;</td><td>0.506</td></tr><tr><td>&quot;ours&quot;</td><td>3</td><td>&quot;f1&quot;</td><td>0.204</td></tr><tr><td>&quot;ours&quot;</td><td>3</td><td>&quot;support&quot;</td><td>748.0</td></tr><tr><td>&quot;ours&quot;</td><td>4</td><td>&quot;accuracy&quot;</td><td>0.221</td></tr><tr><td>&quot;ours&quot;</td><td>4</td><td>&quot;auc&quot;</td><td>0.513</td></tr><tr><td>&quot;ours&quot;</td><td>4</td><td>&quot;f1&quot;</td><td>0.224</td></tr><tr><td>&quot;ours&quot;</td><td>4</td><td>&quot;support&quot;</td><td>829.0</td></tr><tr><td>&quot;ours&quot;</td><td>5</td><td>&quot;accuracy&quot;</td><td>0.323</td></tr><tr><td>&quot;ours&quot;</td><td>5</td><td>&quot;auc&quot;</td><td>0.578</td></tr><tr><td>&quot;ours&quot;</td><td>5</td><td>&quot;f1&quot;</td><td>0.331</td></tr><tr><td>&quot;ours&quot;</td><td>5</td><td>&quot;support&quot;</td><td>838.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (20, 4)\n",
       "┌───────┬───────┬──────────┬───────┐\n",
       "│ model ┆ class ┆ metric   ┆ value │\n",
       "│ ---   ┆ ---   ┆ ---      ┆ ---   │\n",
       "│ str   ┆ i64   ┆ str      ┆ f64   │\n",
       "╞═══════╪═══════╪══════════╪═══════╡\n",
       "│ ours  ┆ 1     ┆ accuracy ┆ 0.336 │\n",
       "│ ours  ┆ 1     ┆ auc      ┆ 0.584 │\n",
       "│ ours  ┆ 1     ┆ f1       ┆ 0.332 │\n",
       "│ ours  ┆ 1     ┆ support  ┆ 755.0 │\n",
       "│ ours  ┆ 2     ┆ accuracy ┆ 0.22  │\n",
       "│ ours  ┆ 2     ┆ auc      ┆ 0.513 │\n",
       "│ ours  ┆ 2     ┆ f1       ┆ 0.212 │\n",
       "│ ours  ┆ 2     ┆ support  ┆ 697.0 │\n",
       "│ ours  ┆ 3     ┆ accuracy ┆ 0.208 │\n",
       "│ ours  ┆ 3     ┆ auc      ┆ 0.506 │\n",
       "│ ours  ┆ 3     ┆ f1       ┆ 0.204 │\n",
       "│ ours  ┆ 3     ┆ support  ┆ 748.0 │\n",
       "│ ours  ┆ 4     ┆ accuracy ┆ 0.221 │\n",
       "│ ours  ┆ 4     ┆ auc      ┆ 0.513 │\n",
       "│ ours  ┆ 4     ┆ f1       ┆ 0.224 │\n",
       "│ ours  ┆ 4     ┆ support  ┆ 829.0 │\n",
       "│ ours  ┆ 5     ┆ accuracy ┆ 0.323 │\n",
       "│ ours  ┆ 5     ┆ auc      ┆ 0.578 │\n",
       "│ ours  ┆ 5     ┆ f1       ┆ 0.331 │\n",
       "│ ours  ┆ 5     ┆ support  ┆ 838.0 │\n",
       "└───────┴───────┴──────────┴───────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "def evaluate_one_window(window):\n",
    "\n",
    "    # get the X (features) and t (target)\n",
    "    test_data = pl.read_ipc(f\"data/test_{window}.feather\", memory_map=False)\n",
    "    train_data = pl.read_ipc(f\"data/train_{window}.feather\", memory_map=False)\n",
    "\n",
    "    # get the cutoff\n",
    "    y_all = pl.read_ipc(\n",
    "        \"data/cutoff.feather\",\n",
    "        memory_map=False\n",
    "    )\n",
    "\n",
    "    cutoff = (\n",
    "        y_all.select(c.docid_idx, y=c.y_car_c5_call_0_21_std, t=c.t_car_c5_call_0_21_std)\n",
    "        .join(train_data, on=\"docid_idx\", how=\"semi\")[\"y\"]\n",
    "        .qcut(5)\n",
    "        .unique()\n",
    "        .to_list()\n",
    "    )\n",
    "    cutoff = sorted([float(cut.split(\",\")[1].strip(\"]\")) for cut in cutoff[:-1]])\n",
    "\n",
    "    # get the yt\n",
    "    yt = (\n",
    "        y_all.join(test_data, on=\"docid_idx\")\n",
    "        .select(\n",
    "            c.docid_idx,\n",
    "            # y=c.y_car_c5_call_0_21_std.cut(cutoff, labels=[\"1\", \"2\", \"3\", \"4\", \"5\"]).cast(pl.Int32),\n",
    "            y=c.y_car_c5_call_0_21_std.qcut(5, labels=[\"1\", \"2\", \"3\", \"4\", \"5\"]).cast(pl.Int32),\n",
    "            t=c.rank,\n",
    "            y_num=c.y_car_c5_call_0_21_std,\n",
    "            t_num=c.t_car_c5_call_0_21_std\n",
    "        )\n",
    "        .with_columns(model=lit(\"ours\"), split_id=lit(window))\n",
    "    )\n",
    "\n",
    "    # evaluate the performance\n",
    "    results_df = evaluate_classification(yt, [\"ours\"])\n",
    "\n",
    "    return yt, results_df\n",
    "\n",
    "def evaluate_all_windows(windows):\n",
    "    yt_all = []\n",
    "    results_df = []\n",
    "    for split_id in windows:\n",
    "        yt, results_df_window = evaluate_one_window(split_id)\n",
    "        results_df.append(results_df_window)\n",
    "        yt_all.append(yt)\n",
    "\n",
    "    results_df = pl.concat(results_df)\n",
    "    yt_all = pl.concat(yt_all)\n",
    "\n",
    "    # average across windows\n",
    "    results_df = (\n",
    "        results_df\n",
    "        .group_by(c.model, c('class'), c.metric)\n",
    "        .agg(value=c.value.mean().round(3))\n",
    "        .with_columns(value=pl.when(c.metric==\"n_instances\").then(c.value*4).otherwise(c.value))\n",
    "        .with_columns(value=pl.when(c.metric==\"support\").then(c.value*4).otherwise(c.value))\n",
    "        .sort(['model', 'class', 'metric'])\n",
    "    )\n",
    "\n",
    "    return yt_all, results_df\n",
    "\n",
    "\n",
    "windows = ['22q1', '22q2', '22q3', '22q4']\n",
    "yt_all, results_df = evaluate_all_windows(windows)\n",
    "\n",
    "# print overall metrics\n",
    "results_df.filter(c('class').is_null())\n",
    "\n",
    "# print per-class metrics\n",
    "results_df.filter(c('class').is_not_null())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect and parse prediction results from all finetuned or non-finetuned LLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yu/App/python-env/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yu/App/python-env/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yu/App/python-env/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yu/App/python-env/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yu/App/python-env/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yu/App/python-env/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yu/App/python-env/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yu/App/python-env/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yu/App/python-env/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (24, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>class</th><th>metric</th><th>value</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>null</td><td>&quot;accuracy&quot;</td><td>0.209</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>null</td><td>&quot;macro_auc&quot;</td><td>0.506</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>null</td><td>&quot;macro_f1&quot;</td><td>0.208</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>null</td><td>&quot;micro_auc&quot;</td><td>0.506</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>null</td><td>&quot;micro_f1&quot;</td><td>0.209</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>null</td><td>&quot;n_instances&quot;</td><td>3864.0</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>null</td><td>&quot;accuracy&quot;</td><td>0.206</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>null</td><td>&quot;macro_auc&quot;</td><td>0.501</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>null</td><td>&quot;macro_f1&quot;</td><td>0.129</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>null</td><td>&quot;micro_auc&quot;</td><td>0.504</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>null</td><td>&quot;micro_f1&quot;</td><td>0.206</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>null</td><td>&quot;n_instances&quot;</td><td>3731.0</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>null</td><td>&quot;accuracy&quot;</td><td>0.212</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>null</td><td>&quot;macro_auc&quot;</td><td>0.507</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>null</td><td>&quot;macro_f1&quot;</td><td>0.21</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>null</td><td>&quot;micro_auc&quot;</td><td>0.507</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>null</td><td>&quot;micro_f1&quot;</td><td>0.212</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>null</td><td>&quot;n_instances&quot;</td><td>3866.0</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>null</td><td>&quot;accuracy&quot;</td><td>0.211</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>null</td><td>&quot;macro_auc&quot;</td><td>0.502</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>null</td><td>&quot;macro_f1&quot;</td><td>0.145</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>null</td><td>&quot;micro_auc&quot;</td><td>0.507</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>null</td><td>&quot;micro_f1&quot;</td><td>0.211</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>null</td><td>&quot;n_instances&quot;</td><td>3839.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (24, 4)\n",
       "┌──────────────────────┬───────┬─────────────┬────────┐\n",
       "│ model                ┆ class ┆ metric      ┆ value  │\n",
       "│ ---                  ┆ ---   ┆ ---         ┆ ---    │\n",
       "│ str                  ┆ i64   ┆ str         ┆ f64    │\n",
       "╞══════════════════════╪═══════╪═════════════╪════════╡\n",
       "│ llama-3.1_ft_frtxt   ┆ null  ┆ accuracy    ┆ 0.209  │\n",
       "│ llama-3.1_ft_frtxt   ┆ null  ┆ macro_auc   ┆ 0.506  │\n",
       "│ llama-3.1_ft_frtxt   ┆ null  ┆ macro_f1    ┆ 0.208  │\n",
       "│ llama-3.1_ft_frtxt   ┆ null  ┆ micro_auc   ┆ 0.506  │\n",
       "│ llama-3.1_ft_frtxt   ┆ null  ┆ micro_f1    ┆ 0.209  │\n",
       "│ llama-3.1_ft_frtxt   ┆ null  ┆ n_instances ┆ 3864.0 │\n",
       "│ llama-3.1_noft_frtxt ┆ null  ┆ accuracy    ┆ 0.206  │\n",
       "│ llama-3.1_noft_frtxt ┆ null  ┆ macro_auc   ┆ 0.501  │\n",
       "│ llama-3.1_noft_frtxt ┆ null  ┆ macro_f1    ┆ 0.129  │\n",
       "│ llama-3.1_noft_frtxt ┆ null  ┆ micro_auc   ┆ 0.504  │\n",
       "│ llama-3.1_noft_frtxt ┆ null  ┆ micro_f1    ┆ 0.206  │\n",
       "│ llama-3.1_noft_frtxt ┆ null  ┆ n_instances ┆ 3731.0 │\n",
       "│ mistral_ft_frtxt     ┆ null  ┆ accuracy    ┆ 0.212  │\n",
       "│ mistral_ft_frtxt     ┆ null  ┆ macro_auc   ┆ 0.507  │\n",
       "│ mistral_ft_frtxt     ┆ null  ┆ macro_f1    ┆ 0.21   │\n",
       "│ mistral_ft_frtxt     ┆ null  ┆ micro_auc   ┆ 0.507  │\n",
       "│ mistral_ft_frtxt     ┆ null  ┆ micro_f1    ┆ 0.212  │\n",
       "│ mistral_ft_frtxt     ┆ null  ┆ n_instances ┆ 3866.0 │\n",
       "│ mistral_noft_frtxt   ┆ null  ┆ accuracy    ┆ 0.211  │\n",
       "│ mistral_noft_frtxt   ┆ null  ┆ macro_auc   ┆ 0.502  │\n",
       "│ mistral_noft_frtxt   ┆ null  ┆ macro_f1    ┆ 0.145  │\n",
       "│ mistral_noft_frtxt   ┆ null  ┆ micro_auc   ┆ 0.507  │\n",
       "│ mistral_noft_frtxt   ┆ null  ┆ micro_f1    ┆ 0.211  │\n",
       "│ mistral_noft_frtxt   ┆ null  ┆ n_instances ┆ 3839.0 │\n",
       "└──────────────────────┴───────┴─────────────┴────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (80, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>class</th><th>metric</th><th>value</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>1</td><td>&quot;accuracy&quot;</td><td>0.237</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>1</td><td>&quot;auc&quot;</td><td>0.52</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>1</td><td>&quot;f1&quot;</td><td>0.23</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>1</td><td>&quot;support&quot;</td><td>754.0</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>2</td><td>&quot;accuracy&quot;</td><td>0.207</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>2</td><td>&quot;auc&quot;</td><td>0.498</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>2</td><td>&quot;f1&quot;</td><td>0.193</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>2</td><td>&quot;support&quot;</td><td>696.0</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>3</td><td>&quot;accuracy&quot;</td><td>0.215</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>3</td><td>&quot;auc&quot;</td><td>0.492</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>3</td><td>&quot;f1&quot;</td><td>0.197</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>3</td><td>&quot;support&quot;</td><td>747.0</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>4</td><td>&quot;accuracy&quot;</td><td>0.176</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>4</td><td>&quot;auc&quot;</td><td>0.5</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>4</td><td>&quot;f1&quot;</td><td>0.19</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>4</td><td>&quot;support&quot;</td><td>829.0</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>5</td><td>&quot;accuracy&quot;</td><td>0.215</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>5</td><td>&quot;auc&quot;</td><td>0.521</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>5</td><td>&quot;f1&quot;</td><td>0.229</td></tr><tr><td>&quot;llama-3.1_ft_frtxt&quot;</td><td>5</td><td>&quot;support&quot;</td><td>838.0</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>1</td><td>&quot;accuracy&quot;</td><td>0.0</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>1</td><td>&quot;auc&quot;</td><td>0.5</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>1</td><td>&quot;f1&quot;</td><td>0.0</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>1</td><td>&quot;support&quot;</td><td>727.0</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>2</td><td>&quot;accuracy&quot;</td><td>0.0</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>2</td><td>&quot;auc&quot;</td><td>0.499</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>2</td><td>&quot;f1&quot;</td><td>0.0</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>2</td><td>&quot;support&quot;</td><td>667.0</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>3</td><td>&quot;accuracy&quot;</td><td>0.229</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>3</td><td>&quot;auc&quot;</td><td>0.49</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>3</td><td>&quot;f1&quot;</td><td>0.196</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>3</td><td>&quot;support&quot;</td><td>714.0</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>4</td><td>&quot;accuracy&quot;</td><td>0.676</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>4</td><td>&quot;auc&quot;</td><td>0.503</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>4</td><td>&quot;f1&quot;</td><td>0.321</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>4</td><td>&quot;support&quot;</td><td>805.0</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>5</td><td>&quot;accuracy&quot;</td><td>0.099</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>5</td><td>&quot;auc&quot;</td><td>0.51</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>5</td><td>&quot;f1&quot;</td><td>0.129</td></tr><tr><td>&quot;llama-3.1_noft_frtxt&quot;</td><td>5</td><td>&quot;support&quot;</td><td>818.0</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>1</td><td>&quot;accuracy&quot;</td><td>0.234</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>1</td><td>&quot;auc&quot;</td><td>0.52</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>1</td><td>&quot;f1&quot;</td><td>0.23</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>1</td><td>&quot;support&quot;</td><td>755.0</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>2</td><td>&quot;accuracy&quot;</td><td>0.199</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>2</td><td>&quot;auc&quot;</td><td>0.502</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>2</td><td>&quot;f1&quot;</td><td>0.191</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>2</td><td>&quot;support&quot;</td><td>697.0</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>3</td><td>&quot;accuracy&quot;</td><td>0.172</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>3</td><td>&quot;auc&quot;</td><td>0.483</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>3</td><td>&quot;f1&quot;</td><td>0.169</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>3</td><td>&quot;support&quot;</td><td>747.0</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>4</td><td>&quot;accuracy&quot;</td><td>0.224</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>4</td><td>&quot;auc&quot;</td><td>0.512</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>4</td><td>&quot;f1&quot;</td><td>0.225</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>4</td><td>&quot;support&quot;</td><td>829.0</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>5</td><td>&quot;accuracy&quot;</td><td>0.227</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>5</td><td>&quot;auc&quot;</td><td>0.519</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>5</td><td>&quot;f1&quot;</td><td>0.235</td></tr><tr><td>&quot;mistral_ft_frtxt&quot;</td><td>5</td><td>&quot;support&quot;</td><td>838.0</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>1</td><td>&quot;accuracy&quot;</td><td>0.0</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>1</td><td>&quot;auc&quot;</td><td>0.5</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>1</td><td>&quot;f1&quot;</td><td>0.0</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>1</td><td>&quot;support&quot;</td><td>745.0</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>2</td><td>&quot;accuracy&quot;</td><td>0.004</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>2</td><td>&quot;auc&quot;</td><td>0.5</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>2</td><td>&quot;f1&quot;</td><td>0.008</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>2</td><td>&quot;support&quot;</td><td>692.0</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>3</td><td>&quot;accuracy&quot;</td><td>0.131</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>3</td><td>&quot;auc&quot;</td><td>0.484</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>3</td><td>&quot;f1&quot;</td><td>0.143</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>3</td><td>&quot;support&quot;</td><td>744.0</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>4</td><td>&quot;accuracy&quot;</td><td>0.58</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>4</td><td>&quot;auc&quot;</td><td>0.507</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>4</td><td>&quot;f1&quot;</td><td>0.311</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>4</td><td>&quot;support&quot;</td><td>821.0</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>5</td><td>&quot;accuracy&quot;</td><td>0.298</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>5</td><td>&quot;auc&quot;</td><td>0.519</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>5</td><td>&quot;f1&quot;</td><td>0.264</td></tr><tr><td>&quot;mistral_noft_frtxt&quot;</td><td>5</td><td>&quot;support&quot;</td><td>837.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (80, 4)\n",
       "┌──────────────────────┬───────┬──────────┬───────┐\n",
       "│ model                ┆ class ┆ metric   ┆ value │\n",
       "│ ---                  ┆ ---   ┆ ---      ┆ ---   │\n",
       "│ str                  ┆ i64   ┆ str      ┆ f64   │\n",
       "╞══════════════════════╪═══════╪══════════╪═══════╡\n",
       "│ llama-3.1_ft_frtxt   ┆ 1     ┆ accuracy ┆ 0.237 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 1     ┆ auc      ┆ 0.52  │\n",
       "│ llama-3.1_ft_frtxt   ┆ 1     ┆ f1       ┆ 0.23  │\n",
       "│ llama-3.1_ft_frtxt   ┆ 1     ┆ support  ┆ 754.0 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 2     ┆ accuracy ┆ 0.207 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 2     ┆ auc      ┆ 0.498 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 2     ┆ f1       ┆ 0.193 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 2     ┆ support  ┆ 696.0 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 3     ┆ accuracy ┆ 0.215 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 3     ┆ auc      ┆ 0.492 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 3     ┆ f1       ┆ 0.197 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 3     ┆ support  ┆ 747.0 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 4     ┆ accuracy ┆ 0.176 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 4     ┆ auc      ┆ 0.5   │\n",
       "│ llama-3.1_ft_frtxt   ┆ 4     ┆ f1       ┆ 0.19  │\n",
       "│ llama-3.1_ft_frtxt   ┆ 4     ┆ support  ┆ 829.0 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 5     ┆ accuracy ┆ 0.215 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 5     ┆ auc      ┆ 0.521 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 5     ┆ f1       ┆ 0.229 │\n",
       "│ llama-3.1_ft_frtxt   ┆ 5     ┆ support  ┆ 838.0 │\n",
       "│ llama-3.1_noft_frtxt ┆ 1     ┆ accuracy ┆ 0.0   │\n",
       "│ llama-3.1_noft_frtxt ┆ 1     ┆ auc      ┆ 0.5   │\n",
       "│ llama-3.1_noft_frtxt ┆ 1     ┆ f1       ┆ 0.0   │\n",
       "│ llama-3.1_noft_frtxt ┆ 1     ┆ support  ┆ 727.0 │\n",
       "│ llama-3.1_noft_frtxt ┆ 2     ┆ accuracy ┆ 0.0   │\n",
       "│ llama-3.1_noft_frtxt ┆ 2     ┆ auc      ┆ 0.499 │\n",
       "│ llama-3.1_noft_frtxt ┆ 2     ┆ f1       ┆ 0.0   │\n",
       "│ llama-3.1_noft_frtxt ┆ 2     ┆ support  ┆ 667.0 │\n",
       "│ llama-3.1_noft_frtxt ┆ 3     ┆ accuracy ┆ 0.229 │\n",
       "│ llama-3.1_noft_frtxt ┆ 3     ┆ auc      ┆ 0.49  │\n",
       "│ llama-3.1_noft_frtxt ┆ 3     ┆ f1       ┆ 0.196 │\n",
       "│ llama-3.1_noft_frtxt ┆ 3     ┆ support  ┆ 714.0 │\n",
       "│ llama-3.1_noft_frtxt ┆ 4     ┆ accuracy ┆ 0.676 │\n",
       "│ llama-3.1_noft_frtxt ┆ 4     ┆ auc      ┆ 0.503 │\n",
       "│ llama-3.1_noft_frtxt ┆ 4     ┆ f1       ┆ 0.321 │\n",
       "│ llama-3.1_noft_frtxt ┆ 4     ┆ support  ┆ 805.0 │\n",
       "│ llama-3.1_noft_frtxt ┆ 5     ┆ accuracy ┆ 0.099 │\n",
       "│ llama-3.1_noft_frtxt ┆ 5     ┆ auc      ┆ 0.51  │\n",
       "│ llama-3.1_noft_frtxt ┆ 5     ┆ f1       ┆ 0.129 │\n",
       "│ llama-3.1_noft_frtxt ┆ 5     ┆ support  ┆ 818.0 │\n",
       "│ mistral_ft_frtxt     ┆ 1     ┆ accuracy ┆ 0.234 │\n",
       "│ mistral_ft_frtxt     ┆ 1     ┆ auc      ┆ 0.52  │\n",
       "│ mistral_ft_frtxt     ┆ 1     ┆ f1       ┆ 0.23  │\n",
       "│ mistral_ft_frtxt     ┆ 1     ┆ support  ┆ 755.0 │\n",
       "│ mistral_ft_frtxt     ┆ 2     ┆ accuracy ┆ 0.199 │\n",
       "│ mistral_ft_frtxt     ┆ 2     ┆ auc      ┆ 0.502 │\n",
       "│ mistral_ft_frtxt     ┆ 2     ┆ f1       ┆ 0.191 │\n",
       "│ mistral_ft_frtxt     ┆ 2     ┆ support  ┆ 697.0 │\n",
       "│ mistral_ft_frtxt     ┆ 3     ┆ accuracy ┆ 0.172 │\n",
       "│ mistral_ft_frtxt     ┆ 3     ┆ auc      ┆ 0.483 │\n",
       "│ mistral_ft_frtxt     ┆ 3     ┆ f1       ┆ 0.169 │\n",
       "│ mistral_ft_frtxt     ┆ 3     ┆ support  ┆ 747.0 │\n",
       "│ mistral_ft_frtxt     ┆ 4     ┆ accuracy ┆ 0.224 │\n",
       "│ mistral_ft_frtxt     ┆ 4     ┆ auc      ┆ 0.512 │\n",
       "│ mistral_ft_frtxt     ┆ 4     ┆ f1       ┆ 0.225 │\n",
       "│ mistral_ft_frtxt     ┆ 4     ┆ support  ┆ 829.0 │\n",
       "│ mistral_ft_frtxt     ┆ 5     ┆ accuracy ┆ 0.227 │\n",
       "│ mistral_ft_frtxt     ┆ 5     ┆ auc      ┆ 0.519 │\n",
       "│ mistral_ft_frtxt     ┆ 5     ┆ f1       ┆ 0.235 │\n",
       "│ mistral_ft_frtxt     ┆ 5     ┆ support  ┆ 838.0 │\n",
       "│ mistral_noft_frtxt   ┆ 1     ┆ accuracy ┆ 0.0   │\n",
       "│ mistral_noft_frtxt   ┆ 1     ┆ auc      ┆ 0.5   │\n",
       "│ mistral_noft_frtxt   ┆ 1     ┆ f1       ┆ 0.0   │\n",
       "│ mistral_noft_frtxt   ┆ 1     ┆ support  ┆ 745.0 │\n",
       "│ mistral_noft_frtxt   ┆ 2     ┆ accuracy ┆ 0.004 │\n",
       "│ mistral_noft_frtxt   ┆ 2     ┆ auc      ┆ 0.5   │\n",
       "│ mistral_noft_frtxt   ┆ 2     ┆ f1       ┆ 0.008 │\n",
       "│ mistral_noft_frtxt   ┆ 2     ┆ support  ┆ 692.0 │\n",
       "│ mistral_noft_frtxt   ┆ 3     ┆ accuracy ┆ 0.131 │\n",
       "│ mistral_noft_frtxt   ┆ 3     ┆ auc      ┆ 0.484 │\n",
       "│ mistral_noft_frtxt   ┆ 3     ┆ f1       ┆ 0.143 │\n",
       "│ mistral_noft_frtxt   ┆ 3     ┆ support  ┆ 744.0 │\n",
       "│ mistral_noft_frtxt   ┆ 4     ┆ accuracy ┆ 0.58  │\n",
       "│ mistral_noft_frtxt   ┆ 4     ┆ auc      ┆ 0.507 │\n",
       "│ mistral_noft_frtxt   ┆ 4     ┆ f1       ┆ 0.311 │\n",
       "│ mistral_noft_frtxt   ┆ 4     ┆ support  ┆ 821.0 │\n",
       "│ mistral_noft_frtxt   ┆ 5     ┆ accuracy ┆ 0.298 │\n",
       "│ mistral_noft_frtxt   ┆ 5     ┆ auc      ┆ 0.519 │\n",
       "│ mistral_noft_frtxt   ┆ 5     ┆ f1       ┆ 0.264 │\n",
       "│ mistral_noft_frtxt   ┆ 5     ┆ support  ┆ 837.0 │\n",
       "└──────────────────────┴───────┴──────────┴───────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all the LLM results\n",
    "def load_llm_results():\n",
    "    yt = []\n",
    "    metrics = []\n",
    "\n",
    "    for model in ['llama-3.1', 'mistral']:\n",
    "        for ft in ['ft', 'noft']:\n",
    "            for split in ['22q1', '22q2', '22q3', '22q4']:\n",
    "                t = (\n",
    "                    pl.read_ipc(f\"data/test_{split}.feather\", columns=['docid_idx', 'rank'])\n",
    "                    .rename({\"rank\": \"t\"})\n",
    "                    .with_columns(c.docid_idx.cast(pl.Int64))\n",
    "                )\n",
    "                \n",
    "                # get yt for one split\n",
    "                yt_one_split = (\n",
    "                    pl.read_ipc(f\"saved_results/results_{model}_{ft}_frtxt_{split}.feather\")\n",
    "                    .with_columns(model=lit(f\"{model}_{ft}_frtxt\"), split_id=lit(split))\n",
    "                    .unique(subset=['docid_idx'])\n",
    "                    # parse the generated text into a score\n",
    "                    .with_columns(y=c.generated_text.str.extract(r\"(?i)Score:?\\s*([\\d\\.\\d]+)\").cast(pl.Float32).round(0).cast(pl.Int32))\n",
    "                    # if the score doesn't fall into the range of 1-5, set it to null\n",
    "                    .with_columns(y=pl.when(c.y.is_in(range(1, 6))).then(c.y).otherwise(None))\n",
    "                    # join with the ground truth\n",
    "                    .join(t, on='docid_idx', how='inner')\n",
    "                )\n",
    "                yt.append(yt_one_split)\n",
    "\n",
    "                # get metrics for one split\n",
    "                metrics_one_split = evaluate_classification(yt_one_split, [f\"{model}_{ft}_frtxt\"])\n",
    "                metrics.append(metrics_one_split)\n",
    "\n",
    "    metrics = pl.concat(metrics)\n",
    "    yt = pl.concat(yt)\n",
    "\n",
    "    return yt, metrics\n",
    "\n",
    "# get yt and metrics\n",
    "yt, metrics = load_llm_results()\n",
    "\n",
    "# average across splits\n",
    "avg_metrics = (\n",
    "    metrics\n",
    "    .group_by(c.model, c('class'), c.metric)\n",
    "    .agg(value=c.value.mean().round(3))\n",
    "    .with_columns(value=pl.when(c.metric==\"n_instances\").then(c.value*4).otherwise(c.value))\n",
    "    .with_columns(value=pl.when(c.metric==\"support\").then(c.value*4).otherwise(c.value))\n",
    "    .sort(['model', 'class', 'metric'])\n",
    ")\n",
    "\n",
    "# print overall metrics\n",
    "avg_metrics.filter(c('class').is_null())\n",
    "\n",
    "# print per-class metrics\n",
    "avg_metrics.filter(c('class').is_not_null())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
