{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import torch\n",
    "from datasets import Dataset, load_from_disk\n",
    "from polars import col as c\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
    "                          BitsAndBytesConfig, TextStreamer, TrainingArguments)\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "\n",
    "# set working directory\n",
    "os.chdir('/home/yuzhu/synology/projects/Call/call/code/v4/reproduce-finetune')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_or_inference=\"finetune\"\n",
    "unsloth=True\n",
    "model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"  # \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"  # \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "max_seq_length=15000\n",
    "max_new_tokens=1000\n",
    "chat_template=\"llama-3.1\"  # \"mistral\" or \"llama-3.1\"\n",
    "train_data_path=\"data/train_data.parquet\"\n",
    "test_data_path=\"data/test_data.parquet\"\n",
    "saved_model_name=\"saved_model/mistral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the huggingface dataset\n",
    "train_data = pl.read_parquet(train_data_path)\n",
    "test_data = pl.read_parquet(test_data_path)\n",
    "logging.info(f\"Read data from {train_data_path} and {test_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we print the first row of the training and testing dataset. Note that the `rank` column is the target variable. It's a number between 1 and 5.\n",
    "\n",
    "Also note that we create `rank` by classifying the PEAD (measured as cumulative abnormal returns, which is the `car_c5_call_0_21` column) into **equal-sized** quintiles.\n",
    "\n",
    "The `docid` column is the unique identifier for each earnings call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 26)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>docid</th><th>transcriptid</th><th>gvkey</th><th>rdq</th><th>is_sp500</th><th>is_r2k</th><th>car_c5_call_0_21</th><th>car_c5_call_0_0</th><th>car_c5_call_0_3</th><th>fund_0_90</th><th>inst_tr2_0_90</th><th>revision_scaled_by_price_90</th><th>demand_retail_3</th><th>sue3</th><th>vol_call_m21_m1</th><th>mcap</th><th>bm</th><th>roa</th><th>debt_assets</th><th>medest</th><th>numest</th><th>stdest</th><th>turnover_ma21</th><th>volume_ma21</th><th>rank</th><th>text</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>date</td><td>bool</td><td>bool</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>cat</td><td>str</td></tr></thead><tbody><tr><td>&quot;001004-2020-12-17&quot;</td><td>2164018</td><td>&quot;001004&quot;</td><td>2020-12-17</td><td>false</td><td>true</td><td>-0.027868</td><td>0.023304</td><td>0.039343</td><td>294.885986</td><td>566.546095</td><td>0.011817</td><td>0.612524</td><td>0.004582</td><td>0.036532</td><td>6.908989</td><td>1.250186</td><td>0.071822</td><td>0.505965</td><td>0.18</td><td>5.0</td><td>0.046693</td><td>0.009553</td><td>337234.428571</td><td>&quot;2&quot;</td><td>&quot;[Management Discussion]:\n",
       "&quot;&quot;&quot;Beâ€¦</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 26)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ docid       â”† transcript â”† gvkey  â”† rdq        â”† â€¦ â”† turnover_m â”† volume_ma2 â”† rank â”† text       â”‚\n",
       "â”‚ ---         â”† id         â”† ---    â”† ---        â”†   â”† a21        â”† 1          â”† ---  â”† ---        â”‚\n",
       "â”‚ str         â”† ---        â”† str    â”† date       â”†   â”† ---        â”† ---        â”† cat  â”† str        â”‚\n",
       "â”‚             â”† i64        â”†        â”†            â”†   â”† f64        â”† f64        â”†      â”†            â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 001004-2020 â”† 2164018    â”† 001004 â”† 2020-12-17 â”† â€¦ â”† 0.009553   â”† 337234.428 â”† 2    â”† [Managemen â”‚\n",
       "â”‚ -12-17      â”†            â”†        â”†            â”†   â”†            â”† 571        â”†      â”† t Discussi â”‚\n",
       "â”‚             â”†            â”†        â”†            â”†   â”†            â”†            â”†      â”† on]:       â”‚\n",
       "â”‚             â”†            â”†        â”†            â”†   â”†            â”†            â”†      â”† \"\"\"Beâ€¦     â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first row of train_data\n",
    "train_data.head(1)\n",
    "\n",
    "# print first row of test_data\n",
    "# test_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct prompts using the training and testing data. The output consists of two huggingface datasets: `train_dataset` and `test_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b715e11290344414b585c2ccbf059cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6a79e58c1f47eda1da2e20f57e4a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dca8047dd8d46139928cd16fda70a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a34d392a0d94d518252ab7ee17223bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=chat_template,\n",
    "    map_eos_token=True,  # e.g., maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "\n",
    "# construct the prompt for training data\n",
    "train_dataset = []\n",
    "for row in train_data.iter_rows(named=True):\n",
    "    # define system\n",
    "    user_content = f\"\"\"You are a financial analyst. You will be given an earnings call transcript of a company and a few financial ratios. Your task is to predict whether the earnings call will have positive or negative impact on the future stock return. Please answer by typing a score between 1 and 5, where 1 is the least positive and 5 is the most positive. Your answer must start with \"Score:\". Please do not concentrate your predictions on the same score, i.e., the number of stocks falling to each score should be balanced. \n",
    "The earnings call may contain three parts: Management Discussion, Questions from Analysts, and Answers from Management. The \"Management Discussion\" section is a statement from the management (usually CEOs and CFOs) about the past performance and future prospects of the company. The \"Questions from Analysts part\" is a question from financial analysts and the \"Answers from Management part\" is the response from the management. There may be multiple rounds of questions and answers in a call. Please also note that the management usually uses very positive language when discussing their company, but you should not take it as granted. Pay attention to the questions from the analysts. \n",
    "\n",
    "Now the earnings call transcript begins:\n",
    "{row[\"text\"]}\n",
    "\n",
    "Now the financial ratios begin:\n",
    "- Earnings surprise (normalized by stock price): {row[\"sue3\"]}\n",
    "- Return volatility in the past month: {row[\"vol_call_m21_m1\"]}\n",
    "- Market capitalization (log-transformed): {row[\"mcap\"]}\n",
    "- Book-to-market ratio: {row[\"bm\"]}\n",
    "- Return-on-assets: {row[\"roa\"]}\n",
    "- Debt-to-assets: {row[\"debt_assets\"]}\n",
    "- Median earnings forecast: {row[\"medest\"]}\n",
    "- Number analysts forecast: {row[\"numest\"]}\n",
    "- Standard deviation of earnings forecast: {row[\"stdest\"]}\n",
    "- Turnover in the past month: {row[\"turnover_ma21\"]}\n",
    "- Trading volume in the past month: {row[\"volume_ma21\"]}\"\"\"\n",
    "\n",
    "    # define user and assistant messages\n",
    "    user = {\n",
    "        \"docid\": row[\"docid\"],\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_content.strip(),\n",
    "    }\n",
    "    assistant = {\n",
    "        \"docid\": row[\"docid\"],\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f'Score:{row[\"rank\"].strip()}',\n",
    "    }\n",
    "\n",
    "    # create prompt based on finetune_or_inference\n",
    "    if finetune_or_inference == \"finetune\":\n",
    "        train_dataset.append([user, assistant])\n",
    "    elif finetune_or_inference == \"inference\":\n",
    "        train_dataset.append([user])\n",
    "\n",
    "# convert to HF Dataset\n",
    "train_dataset = Dataset.from_dict({\"chat\": train_dataset})\n",
    "\n",
    "# apply chat template\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: {\n",
    "        \"prompt\": tokenizer.apply_chat_template(\n",
    "            x[\"chat\"], tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "    },\n",
    "    batched=True,\n",
    ")\n",
    "train_dataset_for_inference = train_dataset.map(\n",
    "    lambda x: {\n",
    "        \"input_ids\": tokenizer.apply_chat_template(\n",
    "            x[\"chat\"], tokenize=True, add_generation_prompt=True, return_tensors=\"np\", max_length=max_seq_length\n",
    "        )\n",
    "    },\n",
    "    batched=True,\n",
    ")\n",
    "train_dataset_for_inference.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "# Repeat the same process for test data\n",
    "test_dataset = []\n",
    "for row in test_data.iter_rows(named=True):\n",
    "    # Same code as above for test data\n",
    "    user_content = f\"\"\"You are a financial analyst. You will be given an earnings call transcript of a company and a few financial ratios. Your task is to predict whether the earnings call will have positive or negative impact on the future stock return. Please answer by typing a score between 1 and 5, where 1 is the least positive and 5 is the most positive. Your answer must start with \"Score:\". Please do not concentrate your predictions on the same score, i.e., the number of stocks falling to each score should be balanced. \n",
    "The earnings call may contain three parts: Management Discussion, Questions from Analysts, and Answers from Management. The \"Management Discussion\" section is a statement from the management (usually CEOs and CFOs) about the past performance and future prospects of the company. The \"Questions from Analysts part\" is a question from financial analysts and the \"Answers from Management part\" is the response from the management. There may be multiple rounds of questions and answers in a call. Please also note that the management usually uses very positive language when discussing their company, but you should not take it as granted. Pay attention to the questions from the analysts. \n",
    "\n",
    "Now the earnings call transcript begins:\n",
    "{row[\"text\"]}\n",
    "\n",
    "Now the financial ratios begin:\n",
    "- Earnings surprise (normalized by stock price): {row[\"sue3\"]}\n",
    "- Return volatility in the past month: {row[\"vol_call_m21_m1\"]}\n",
    "- Market capitalization (log-transformed): {row[\"mcap\"]}\n",
    "- Book-to-market ratio: {row[\"bm\"]}\n",
    "- Return-on-assets: {row[\"roa\"]}\n",
    "- Debt-to-assets: {row[\"debt_assets\"]}\n",
    "- Median earnings forecast: {row[\"medest\"]}\n",
    "- Number analysts forecast: {row[\"numest\"]}\n",
    "- Standard deviation of earnings forecast: {row[\"stdest\"]}\n",
    "- Turnover in the past month: {row[\"turnover_ma21\"]}\n",
    "- Trading volume in the past month: {row[\"volume_ma21\"]}\"\"\"\n",
    "\n",
    "    user = {\n",
    "        \"docid\": row[\"docid\"],\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_content,\n",
    "    }\n",
    "    assistant = {\n",
    "        \"docid\": row[\"docid\"],\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f'Score:{row[\"rank\"]}',\n",
    "    }\n",
    "\n",
    "    if finetune_or_inference == \"finetune\":\n",
    "        test_dataset.append([user, assistant])\n",
    "    elif finetune_or_inference == \"inference\":\n",
    "        test_dataset.append([user])\n",
    "\n",
    "test_dataset = Dataset.from_dict({\"chat\": test_dataset})\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda x: {\n",
    "        \"prompt\": tokenizer.apply_chat_template(\n",
    "            x[\"chat\"], tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "    },\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "test_dataset_for_inference = test_dataset.map(\n",
    "    lambda x: {\n",
    "        \"input_ids\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=True, add_generation_prompt=True, return_tensors=\"np\", max_length=max_seq_length)\n",
    "    },\n",
    "    batched=True,\n",
    ")\n",
    "test_dataset_for_inference.set_format(type=\"torch\", columns=[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.5: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA RTX 6000 Ada Generation. Max memory: 47.493 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Not an error, but Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Not an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2024.11.5 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Get model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=torch.bfloat16,  # None for autodetect\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# add LoRA adapter\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        # \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        # \"o_proj\",\n",
    "        # \"gate_proj\",\n",
    "        # \"up_proj\",\n",
    "        # \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=985,\n",
    "    use_rslora=True,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e173c5c3497b43b0b03dd7dc00403fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/6673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d870f4c5dd7f4352ac77b8ff5e92ef29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/1198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# init trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=16,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        # warmup_steps=10,\n",
    "        warmup_ratio=0.1,\n",
    "        max_steps=25,  # set to -1 for full training\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        logging_steps=1,\n",
    "        eval_steps=50,\n",
    "        optim=\"adamw_8bit\",  # adamw_8bit\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=985,\n",
    "        output_dir=f\"saved_model/{chat_template}\",\n",
    "        save_strategy=\"no\",\n",
    "        overwrite_output_dir=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4d7a00027e4b3499f15b894f5306cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b068d269994a1ea729208c88958a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (important) only train on assistant responses!\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only for debugging\n",
    "# tokenizer.decode(trainer.train_dataset[1][\"input_ids\"])\n",
    "# space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "# masked = tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[1][\"labels\"]])\n",
    "# masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 6,673 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 8 | Total steps = 25\n",
      " \"-____-\"     Number of trainable parameters = 3,407,872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 18:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.274500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.364900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.286800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.267600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.249900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.179500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... Done.\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# save the model\n",
    "model.save_pretrained_merged(\n",
    "    saved_model_name,\n",
    "    tokenizer,\n",
    "    save_method=\"lora\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1198 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "  3%|â–         | 34/1198 [07:11<5:10:45, 16.02s/it]"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# if not using streamer\n",
    "results = []\n",
    "for call, input_ids in tqdm(\n",
    "    zip(test_dataset_for_inference[\"chat\"], test_dataset_for_inference[\"input_ids\"]), total=len(test_dataset_for_inference)\n",
    "):\n",
    "\n",
    "    docid = call[0][\"docid\"]\n",
    "    prompt = call[0][\"content\"]\n",
    "    input_length = input_ids.shape[0]  # input_ids is a 1D tensor\n",
    "\n",
    "    # debug only\n",
    "    # if not (input_length >= 5000):\n",
    "    #     logging.info(f\"Skipping \\\"{docid}\\\" ({input_length} tokens)\")\n",
    "    #     continue\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids.unsqueeze(0).to(model.device),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    generated_text = tokenizer.batch_decode(\n",
    "        generated_ids[:, input_length:], skip_special_tokens=True\n",
    "    )[0]\n",
    "\n",
    "    # every output saved as a single file\n",
    "    out = {\"docid\": docid, \"prompt\": prompt, \"response\": generated_text}\n",
    "    out = pl.DataFrame(out)\n",
    "    results.append(out)\n",
    "\n",
    "\n",
    "# at the end, read all the files and save as a feather file\n",
    "results = pl.concat(results, how='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.000\n",
      "\n",
      "Per-class metrics:\n",
      "\n",
      "Class 1:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-score: 0.000\n",
      "Support: 2.0\n",
      "\n",
      "Class 2:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-score: 0.000\n",
      "Support: 0.0\n",
      "\n",
      "Class 3:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-score: 0.000\n",
      "Support: 0.0\n",
      "\n",
      "Class 4:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-score: 0.000\n",
      "Support: 0.0\n",
      "\n",
      "Class 5:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-score: 0.000\n",
      "Support: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuzhu/App/python-env/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuzhu/App/python-env/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuzhu/App/python-env/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# extrat predictions\n",
    "preds = (results\n",
    "    .select(c.docid, pred=c.response.str.extract(r\"(?i)Score:?\\s*(\\d)\").cast(pl.Int64))\n",
    "    .filter(c.pred.is_not_null()))\n",
    "\n",
    "# merge preds with ground truth\n",
    "merged = (test_data.join(preds, on=\"docid\", how=\"inner\")\n",
    "    .select(c.docid, c.pred, target=c.rank.cast(pl.Int64)))\n",
    "\n",
    "t = merged['target'].to_list()\n",
    "y = merged['pred'].to_list()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = accuracy_score(t, y)\n",
    "\n",
    "# Calculate per-class precision, recall, and f1\n",
    "# average=None means it will return scores for each class\n",
    "precision, recall, f1, support = precision_recall_fscore_support(t, y, average=None, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Print results\n",
    "print(f\"Overall accuracy: {accuracy:.3f}\")\n",
    "print(\"\\nPer-class metrics:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\nClass {i+1}:\")\n",
    "    print(f\"Precision: {precision[i]:.3f}\")\n",
    "    print(f\"Recall: {recall[i]:.3f}\")\n",
    "    print(f\"F1-score: {f1[i]:.3f}\")\n",
    "    print(f\"Support: {support[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
